{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUNmybKZQiHi"
      },
      "source": [
        "# **HYBRID MODEL FOR CHURN PREDICTION WITH EXPLAINABLE AI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuuMdoQoQiHs"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_color_codes(\"pastel\")\n",
        "sns.set_style(\"whitegrid\")\n",
        "%matplotlib inline\n",
        "\n",
        "from pyspark.sql import SparkSession, Window\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import sum as Fsum\n",
        "from pyspark.sql.functions import min as Fmin\n",
        "from pyspark.sql.functions import max as Fmax\n",
        "from pyspark.sql.functions import avg, col, concat, count, desc, asc, explode, lit, split, stddev, udf, isnan, when, rank, from_unixtime\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV-Mc2Z_QiHt"
      },
      "outputs": [],
      "source": [
        "# Create spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Sparkify\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdy0QScoQiHt"
      },
      "outputs": [],
      "source": [
        "# Read in full sparkify dataset\n",
        "event_data = \"/content/mini_sparkify_event_data.json\"\n",
        "df = spark.read.json(event_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcBB4FP2QiHt"
      },
      "source": [
        "# Load and Clean Dataset\n",
        "\n",
        "In this notebook, the file name, `mini_sparkify_event_data.json`, will be loaded and cleaned such as handling of invalid or missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0o4z3Y7QiHt"
      },
      "source": [
        "The first five rows of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8sxiAjNQiHt"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5wMEeuJQiHu"
      },
      "source": [
        "Schema information\n",
        "\n",
        "* artist: Artist name (ex. Daft Punk)\n",
        "* auth: User authentication status (ex. Logged)\n",
        "* firstName: User first name (ex. Colin)\n",
        "* gender: Gender (ex. F or M)\n",
        "* itemInSession: Item count in a session (ex. 52)\n",
        "* lastName: User last name (ex. Freeman)\n",
        "* length: Length of song (ex. 223.60771)\n",
        "* level: User plan (ex. paid)\n",
        "* location: User's location (ex. Bakersfield)\n",
        "* method: HTTP method (ex. PUT)\n",
        "* page: Page name (ex. NextSong)\n",
        "* registration: Registration timestamp (unix timestamp) (ex. 1538173362000)\n",
        "* sessionId: Session ID (ex. 29)\n",
        "* song: Song (ex. Harder Better Faster Stronger)\n",
        "* status: HTTP status (ex. 200)\n",
        "* ts: Event timestamp(unix timestamp) (ex. 1538352676000)\n",
        "* userAgent: User's browswer agent (ex. Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0)\n",
        "* userId: User ID (ex. 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKFxnvxlQiHu"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N9OZki8QiHu"
      },
      "source": [
        "## Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DACrOU6UQiHu"
      },
      "source": [
        "Statistics of the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wYrSxvdQiHu"
      },
      "outputs": [],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vogDf1-UQiHu"
      },
      "source": [
        "Statistics of the `artist` column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIdaAiGUQiHu"
      },
      "outputs": [],
      "source": [
        "df.describe('artist').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8U1GfLKQiHu"
      },
      "source": [
        "Statistics of the `sessionId` column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0LedDyoQiHu"
      },
      "outputs": [],
      "source": [
        "df.describe('sessionId').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJeeQMQyQiHu"
      },
      "source": [
        "Statistics of the `userId` column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T39TDIdQiHu"
      },
      "outputs": [],
      "source": [
        "df.describe('userId').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii38oMRqQiHu"
      },
      "source": [
        "Total rows: 286,500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fjGgVS9QiHu"
      },
      "outputs": [],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPCTy65QQiHv"
      },
      "source": [
        "All the `page` events in the dataset:\n",
        "\n",
        "- About\n",
        "- Add Friend\n",
        "- Add to Playlist\n",
        "- Cancel\n",
        "- Cancellation Confirmation: **This even wil be used as a flag of churn**\n",
        "- Downgrade\n",
        "- Error\n",
        "- Help\n",
        "- Home\n",
        "- Login\n",
        "- Logout\n",
        "- NextSong\n",
        "- Register\n",
        "- Roll Advert\n",
        "- Save Settings\n",
        "- Settings\n",
        "- Submit Downgrade\n",
        "- Submit Registration\n",
        "- Submit Upgrade\n",
        "- Thumbs Down\n",
        "- Thumbs Up\n",
        "- Upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQhlGkmKQiHv"
      },
      "source": [
        "`page` kind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQaEZuT3QiHv"
      },
      "outputs": [],
      "source": [
        "df.select(\"page\").dropDuplicates().sort(\"page\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m9b4vPGQiHv"
      },
      "source": [
        "## missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkhcIEUyQiHv"
      },
      "outputs": [],
      "source": [
        "def count_missing(df, col):\n",
        "    \"\"\"\n",
        "    A helper function which count how many missing values in a colum of the dataset.\n",
        "\n",
        "    This function is useful because the data can be either three cases below:\n",
        "\n",
        "    1. NaN\n",
        "    2. Null\n",
        "    3. \"\" (empty string)\n",
        "    \"\"\"\n",
        "    return df.filter((isnan(df[col])) | (df[col].isNull()) | (df[col] == \"\")).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IgXc7nnQiHv"
      },
      "source": [
        "Check how many missing values in each column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuzQN8qiQiHv"
      },
      "outputs": [],
      "source": [
        "print(\"[missing values]\\n\")\n",
        "for col in df.columns:\n",
        "    missing_count = count_missing(df, col)\n",
        "    if missing_count > 0:\n",
        "        print(\"{}: {}\".format(col, missing_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2TP9mjtQiHv"
      },
      "source": [
        "`userId` and `sessionId`\n",
        "\n",
        "If the below Ids are null or empty, delete those rows:\n",
        "\n",
        "* userId\n",
        "* sessionId"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxA1b9ZJQiHv"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id = df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
        "df_without_missing_id = df_without_missing_id.filter(df[\"userId\"] != \"\") # `userId` should not be empty string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzIMKhzCQiHv"
      },
      "outputs": [],
      "source": [
        "print(\"df:                    {}\".format(df.count()))\n",
        "print(\"df_without_missing_id: {}\".format(df_without_missing_id.count())) # no missing values\n",
        "\n",
        "if df.count() == df_without_missing_id.count():\n",
        "    print(\"No missing values with userId and sessionId\")\n",
        "else:\n",
        "    print(\"{} rows have been removed.\".format(df.count() - df_without_missing_id.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhzRr2uqQiHv"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjdiY3phQiHv"
      },
      "source": [
        "Detect number columns and category columns.\n",
        "\n",
        "* num_cols: Number columns (Long or Double)\n",
        "* cat_cols: Category columns (String)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-uPQkalQiHw"
      },
      "outputs": [],
      "source": [
        "num_cols = []\n",
        "cat_cols = []\n",
        "\n",
        "for s in df.schema:\n",
        "    data_type = str(s.dataType)\n",
        "    if data_type == \"StringType\":\n",
        "        cat_cols.append(s.name)\n",
        "\n",
        "    if data_type == \"LongType\" or data_type == \"DoubleType\":\n",
        "        num_cols.append(s.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-0Lv0-VQiHw"
      },
      "outputs": [],
      "source": [
        "num_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh-97jDaQiHw"
      },
      "outputs": [],
      "source": [
        "cat_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyyx5rwmQiHw"
      },
      "source": [
        "## Number columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAR1L2VaQiHw"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id.describe(num_cols).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI-soGaJQiHw"
      },
      "source": [
        "There are three HTTP status codes:\n",
        "\n",
        "* 307: Temporary Redirect\n",
        "* 404: Not Found\n",
        "* 200: OK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG9UAW76QiHw"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id.select(\"status\").dropDuplicates().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31QlMfAwQiHw"
      },
      "source": [
        "### Category columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFih5yTjQiHw"
      },
      "source": [
        "auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPwT4B6vQiHw"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id.select(\"auth\").dropDuplicates().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUBJZwODQiHw"
      },
      "source": [
        "gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z_N0uBrQiHw"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id.select(\"gender\").dropDuplicates().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joCWi-eDQiHx"
      },
      "source": [
        "level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDMH25TsQiHx"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id.select(\"level\").dropDuplicates().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLIn9mFLQiHx"
      },
      "source": [
        "location (only showing top 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQsV4DWnQiHx"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id.select(\"location\").dropDuplicates().show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaR5iagnQiHx"
      },
      "source": [
        "method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UGnmLDbQiHx"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id.select(\"method\").dropDuplicates().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBcoVL1CQiHx"
      },
      "source": [
        "page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De8mLbBFQiHx"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id.select(\"page\").dropDuplicates().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkU_9gDxQiHx"
      },
      "source": [
        "userAgent (only showing top 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CNSGh_AQiHx"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id.select(\"userAgent\").dropDuplicates().show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFLTsnACQiHx"
      },
      "source": [
        "### Define Churn\n",
        "\n",
        "Churn will be defined as when `Cancellation Confirmation` events happen, and users with the events are churned users in this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAN3JnXFQiHx"
      },
      "source": [
        "churn: `Cancellation Confirmation`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiRdkvnwQiHx"
      },
      "outputs": [],
      "source": [
        "df_without_missing_id.filter(\"page = 'Cancellation Confirmation'\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7iD5dfLQiHy"
      },
      "outputs": [],
      "source": [
        "flag_churned_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
        "df_churned = df_without_missing_id.withColumn(\"churned\", flag_churned_event(\"page\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Czd6VvQiHy"
      },
      "source": [
        "churned rate (from total event logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtKp3upTQiHy"
      },
      "outputs": [],
      "source": [
        "churned_rate = df_churned.groupby(\"userId\").agg({\"churned\": \"sum\"}).select(avg(\"sum(churned)\")).collect()[0][\"avg(sum(churned))\"]\n",
        "print(\"churned: {:.2f}%\".format(churned_rate * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diYOfxJNQiHy"
      },
      "outputs": [],
      "source": [
        "df_churned.select([\"userId\", \"gender\", \"level\", \"page\", \"status\", \"ts\", \"churned\"]).show(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EytbJSWdQiHy"
      },
      "outputs": [],
      "source": [
        "windowval = Window.partitionBy(\"userId\").orderBy(asc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
        "df_phase = df_churned.withColumn(\"phase\", Fsum('churned').over(windowval))\n",
        "df_churn = df_phase.withColumn(\"churn\", Fmax('churned').over(Window.partitionBy(\"userId\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM8ud0OpQiHy"
      },
      "outputs": [],
      "source": [
        "df_churn.select([\"userId\", \"gender\", \"level\", \"page\", \"status\", \"ts\", \"churned\", \"phase\", \"churn\"]).show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbyTG31GQiHy"
      },
      "outputs": [],
      "source": [
        "df_churn.filter(df_churn[\"churn\"] == 1).select([\"userId\", \"gender\", \"level\", \"page\", \"status\", \"ts\", \"churned\", \"phase\", \"churn\"]).show(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5osWY0vQiHy"
      },
      "source": [
        "52 userIds were churned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcrNPayYQiHy"
      },
      "outputs": [],
      "source": [
        "churned_user_count = df_churn.filter(df_churn[\"churn\"] == 1).select(\"userId\").dropDuplicates().count()\n",
        "print(\"churned user count: {} (total: {})\".format(churned_user_count, df_churn.count()))\n",
        "print(\"churned user rate: {:.2f}%\".format(churned_user_count / df_churn.count() * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm8Ukp_qQiHy"
      },
      "source": [
        "### Explore Data\n",
        "\n",
        "In this section, data exploration will be done comparing churned users with not churned users, inspecting if there are any big differences between the two groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C5vS9AeQiHy"
      },
      "source": [
        "The below columns will be examined:\n",
        "\n",
        "* artist\n",
        "  * [x] the number of artist\n",
        "* [x] gender: 0 or 1\n",
        "* length\n",
        "  * [x] the total length\n",
        "* [x] level: 0 or 1\n",
        "* page\n",
        "  * [x] the number of `Thumbs Up`\n",
        "  * [x] the number of `Thumbs Down`\n",
        "* song\n",
        "  * [x] the number of song"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRsE6656sQLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN6Da8zYQiHy"
      },
      "source": [
        "Define a common function to convert churn value (0 or 1) to `Not Churn` or `Churn`\n",
        "\n",
        "Both matplotlib and seaborn plot libraries require pandas dataframe, not pyspark dataframe, so I need to convert the pyspark dataframe to pandas one. I do this conversion every time for a small subset of the dataset because if I do this conversion for all the dataset, it takes time and causes an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kjD0Y6mQiHz"
      },
      "outputs": [],
      "source": [
        "func_churn_label = udf(lambda x: 'Churn' if x == 1 else 'Not Churn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF4NS60bQiHz"
      },
      "outputs": [],
      "source": [
        "df_churn_user = df_churn.groupby(\"userId\").max(\"churn\").withColumnRenamed(\"max(churn)\", \"churn\").select([\"userId\", \"churn\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OY9HxEeQiHz"
      },
      "source": [
        "gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MAaL_fkQiHz"
      },
      "outputs": [],
      "source": [
        "pd_gender = df_churn.select([\"userId\", \"gender\", \"churn\"]).withColumn(\"churn\", func_churn_label(\"churn\")).toPandas()\n",
        "pd_gender.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzdl5jagQiHz"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=\"gender\", hue=\"churn\", data=pd_gender);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69pwBDZQiHz"
      },
      "source": [
        "level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CbpaHNhQiHz"
      },
      "outputs": [],
      "source": [
        "pd_level = df_churn.select([\"userId\", \"level\", \"churn\"]).withColumn(\"churn\", func_churn_label(\"churn\")).toPandas()\n",
        "pd_level.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no6wrGsOQiHz"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=\"level\", hue=\"churn\", data=pd_level);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkSOq0Y-QiHz"
      },
      "source": [
        "artist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-34DaQJQiHz"
      },
      "outputs": [],
      "source": [
        "pd_artist = df_churn_user.join(df_churn.groupby(\"userId\") \\\n",
        "                                    .agg({\"artist\": \"count\"}) \\\n",
        "                                    .withColumnRenamed(\"count(artist)\", \"artist_count\"), [\"userId\"]) \\\n",
        "                         .withColumn(\"churn\", func_churn_label(\"churn\")).toPandas()\n",
        "pd_artist.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TkGS1GOQiHz"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=\"churn\", y=\"artist_count\", data=pd_artist);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnPtQLISQiH0"
      },
      "source": [
        "song"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7_jP_HCQiH0"
      },
      "outputs": [],
      "source": [
        "pd_song = df_churn_user.join(df_churn.groupby(\"userId\") \\\n",
        "                                     .agg({\"song\": \"count\"}) \\\n",
        "                                     .withColumnRenamed(\"count(song)\", \"song_count\"), [\"userId\"]) \\\n",
        "                       .withColumn(\"churn\", func_churn_label(\"churn\")).toPandas()\n",
        "pd_song.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-NGhUIjQiH0"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=\"churn\", y=\"song_count\", data=pd_song);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l_aBg0PQiH0"
      },
      "source": [
        "length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFL6qvVxQiH0"
      },
      "outputs": [],
      "source": [
        "pd_length = df_churn_user.join(df_churn.groupby(\"userId\") \\\n",
        "                                       .agg({\"length\": \"sum\"}) \\\n",
        "                                       .withColumnRenamed(\"sum(length)\", \"total_length\"), [\"userId\"]) \\\n",
        "                          .withColumn(\"churn\", func_churn_label(\"churn\")).toPandas()\n",
        "pd_length.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4aKhPAWQiH0"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=\"churn\", y=\"total_length\", data=pd_length);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS4LuyXdQiH0"
      },
      "source": [
        "page: total visits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXi8Qeu4QiH0"
      },
      "outputs": [],
      "source": [
        "pd_visit = df_churn_user.join(df_churn.groupby(\"userId\") \\\n",
        "                                      .count() \\\n",
        "                                      .withColumnRenamed(\"count\", \"visit_count\"), [\"userId\"]) \\\n",
        "                         .withColumn(\"churn\", func_churn_label(\"churn\")).toPandas()\n",
        "pd_visit.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_pNv57zQiH0"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=\"churn\", y=\"visit_count\", data=pd_visit);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECKwN8lSQiH0"
      },
      "source": [
        "page: Thumbs Up / Thumbs Down"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVk2b9ATQiH0"
      },
      "source": [
        "up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUrnXbZPQiH1"
      },
      "outputs": [],
      "source": [
        "pd_up = df_churn_user.join(df_churn.filter((df_churn[\"page\"] == 'Thumbs Up')) \\\n",
        "                                   .groupby(\"userId\") \\\n",
        "                                   .count() \\\n",
        "                                   .withColumnRenamed(\"count\", \"up_count\"), [\"userId\"]) \\\n",
        "                     .withColumn(\"churn\", func_churn_label(\"churn\")).toPandas()\n",
        "pd_up.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSKz0ERXQiH1"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=\"churn\", y=\"up_count\", data=pd_up);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgpKGP-nQiH1"
      },
      "source": [
        "down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ0CEmr5QiH1"
      },
      "outputs": [],
      "source": [
        "pd_down = df_churn_user.join(df_churn.filter((df_churn[\"page\"] == 'Thumbs Down')) \\\n",
        "                                   .groupby(\"userId\") \\\n",
        "                                   .count() \\\n",
        "                                   .withColumnRenamed(\"count\", \"down_count\"), [\"userId\"]) \\\n",
        "                     .withColumn(\"churn\", func_churn_label(\"churn\")).toPandas()\n",
        "pd_down.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r89nCCKDQiH1"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=\"churn\", y=\"down_count\", data=pd_down);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEcar1vOQiH1"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SmLS9yZQiH1"
      },
      "source": [
        "### Feature Engineering Ideas\n",
        "\n",
        "* artist\n",
        "  * [x] the number of artist\n",
        "* [x] gender: 0 or 1\n",
        "* length\n",
        "  * [x] the total length\n",
        "* [x] level: 0 or 1\n",
        "* page\n",
        "  * [x] the number of `Thumbs Up`\n",
        "  * [x] the number of `Thumbs Down`\n",
        "* song\n",
        "  * [x] the number of song"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwt2VhdsQiH1"
      },
      "outputs": [],
      "source": [
        "df_churn.show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcGtv9jEQiH1"
      },
      "source": [
        "Original dataframe to be merged later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKESan42QiH1"
      },
      "outputs": [],
      "source": [
        "df_original = df_churn.groupby('userId').max(\"churn\").withColumnRenamed(\"max(churn)\", \"target\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DiPcNWSQiH1"
      },
      "outputs": [],
      "source": [
        "df_original.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAyHfBIxQiH1"
      },
      "source": [
        "artist count per userId"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3T4Zt5bQiH1"
      },
      "outputs": [],
      "source": [
        "user_artist = df_churn.groupby(\"userId\").agg({\"artist\": \"count\"}).withColumnRenamed(\"count(artist)\", \"artist_count\")\n",
        "user_artist.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ToDht9aQiH1"
      },
      "source": [
        "gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ApSwE7wQiH2"
      },
      "outputs": [],
      "source": [
        "flag_gender = udf(lambda x: 1 if x == \"F\" else 0, IntegerType())\n",
        "df_churn_with_gender = df_churn.withColumn(\"gender\", flag_gender(\"gender\"))\n",
        "df_churn_with_gender.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6CXngn1QiH2"
      },
      "outputs": [],
      "source": [
        "user_gender = df_churn_with_gender.groupby('userId').agg({\"gender\": \"max\"}).withColumnRenamed(\"max(gender)\", \"gender\")\n",
        "user_gender.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KzayIq7QiH2"
      },
      "source": [
        "length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5CWO7FxQiH2"
      },
      "outputs": [],
      "source": [
        "user_length = df_churn.groupby('userId').agg({\"length\": \"sum\"}).withColumnRenamed(\"sum(length)\", \"length\")\n",
        "user_length.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSgZ08BAQiH2"
      },
      "source": [
        "Page\n",
        "\n",
        "* Thumbs Up\n",
        "* Thumbs Down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMwh5vq6QiH2"
      },
      "outputs": [],
      "source": [
        "user_thumbs_up = df_churn.filter(df_churn[\"page\"] == 'Thumbs Up').groupby('userId').count().withColumnRenamed(\"count\", \"thumb_up\")\n",
        "user_thumbs_up.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31_-JJwlQiH2"
      },
      "outputs": [],
      "source": [
        "user_thumbs_down = df_churn.filter(df_churn[\"page\"] == 'Thumbs Down').groupby('userId').count().withColumnRenamed(\"count\", \"thumb_down\")\n",
        "user_thumbs_down.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkNk8gC7QiH2"
      },
      "source": [
        "level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2IRgSyFQiH2"
      },
      "outputs": [],
      "source": [
        "flag_level = udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
        "df_churn_with_level = df_churn.withColumn(\"level\", flag_level(\"level\"))\n",
        "df_churn_with_level.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH7gyCQ7QiH2"
      },
      "outputs": [],
      "source": [
        "user_level = df_churn_with_level.groupby('userId').agg({\"level\": \"max\"}).withColumnRenamed(\"max(level)\", \"level\")\n",
        "user_level.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtGOVhl6QiH2"
      },
      "source": [
        "song count per userId"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SByJ7TrWQiH2"
      },
      "outputs": [],
      "source": [
        "user_song = df_churn.groupby(\"userId\").agg({\"song\": \"count\"}).withColumnRenamed(\"count(song)\", \"song_count\")\n",
        "user_song.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTS-WS4zQiH3"
      },
      "source": [
        "Join all the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in4bdB8NQiH3"
      },
      "outputs": [],
      "source": [
        "merged_df = df_original.join(user_artist, ['userId']) \\\n",
        "    .join(user_gender, ['userId']) \\\n",
        "    .join(user_length, ['userId']) \\\n",
        "    .join(user_level, ['userId']) \\\n",
        "    .join(user_thumbs_up, ['userId']) \\\n",
        "    .join(user_thumbs_down, ['userId']) \\\n",
        "    .join(user_song, ['userId'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbVyO5UNQiH3"
      },
      "outputs": [],
      "source": [
        "merged_df.show(20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the schema of merged_df, which includes column names and data types\n",
        "merged_df.printSchema()\n"
      ],
      "metadata": {
        "id": "QCkCok8fp_dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Cast userId to IntegerType\n",
        "merged_df = merged_df.withColumn(\"userId\", merged_df[\"userId\"].cast(IntegerType()))\n",
        "\n",
        "# Verify the schema to confirm the change\n",
        "merged_df.printSchema()\n"
      ],
      "metadata": {
        "id": "5YVVnkH9qpr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "# List of feature columns to include in Chi-Square Test\n",
        "feature_columns = [ 'userId', 'target', 'artist_count', 'gender', 'length', 'level', 'thumb_up', 'thumb_down', 'song_count']  # Adjust based on actual columns\n",
        "\n",
        "# Assemble the features into a single feature vector\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "df_vector = assembler.transform(merged_df)\n"
      ],
      "metadata": {
        "id": "-ltb74qXowxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Perform Chi-Square Test on the assembled data\n",
        "chi_square_test = ChiSquareTest.test(df_vector, 'features', 'target')\n",
        "chi_square_results = chi_square_test.head()  # Get the first result (there should only be one in the output)\n",
        "\n",
        "# Creating a dictionary to hold the Chi-Square results\n",
        "chi_square_data = {\n",
        "    'Feature': feature_columns,\n",
        "    'Chi-Square Statistic': chi_square_results['statistics'],\n",
        "    'p-value': chi_square_results['pValues'],\n",
        "    'Degrees of Freedom': chi_square_results['degreesOfFreedom']\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a Pandas DataFrame for a tabular view\n",
        "chi_square_df = pd.DataFrame(chi_square_data)\n",
        "\n",
        "# Display the results\n",
        "print(chi_square_df)\n"
      ],
      "metadata": {
        "id": "vwLlx62kq3r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Initialize a Logistic Regression model\n",
        "lr = LogisticRegression(labelCol='target', featuresCol='features')\n",
        "\n",
        "# Evaluation metric\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='target', rawPredictionCol='prediction', metricName='areaUnderROC')\n",
        "\n",
        "# Function to perform SFS\n",
        "def sequential_feature_selection(data, feature_columns, target_col, max_features):\n",
        "    selected_features = []\n",
        "    remaining_features = feature_columns.copy()\n",
        "\n",
        "    for _ in range(max_features):\n",
        "        best_feature = None\n",
        "        best_score = 0\n",
        "\n",
        "        for feature in remaining_features:\n",
        "            # Create the feature vector\n",
        "            current_features = selected_features + [feature]\n",
        "            assembler = VectorAssembler(inputCols=current_features, outputCol='features')\n",
        "            df_vector = assembler.transform(data)\n",
        "\n",
        "            # Train the model\n",
        "            model = lr.fit(df_vector)\n",
        "            predictions = model.transform(df_vector)\n",
        "\n",
        "            # Evaluate the model\n",
        "            score = evaluator.evaluate(predictions)\n",
        "\n",
        "            # Select the best feature based on the score\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_feature = feature\n",
        "\n",
        "        # Update selected and remaining features\n",
        "        if best_feature is not None:\n",
        "            selected_features.append(best_feature)\n",
        "            remaining_features.remove(best_feature)\n",
        "            print(f'Selected feature: {best_feature}, Score: {best_score}')\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "# Define your feature columns and target column\n",
        "feature_columns = [ 'artist_count', 'gender', 'length', 'level', 'thumb_up', 'thumb_down', 'song_count']\n",
        "target_col = 'target'\n",
        "\n",
        "# Perform SFS with a maximum of 3 features\n",
        "best_features = sequential_feature_selection(merged_df, feature_columns, target_col, max_features=9)\n",
        "\n",
        "print(\"Best Features Selected: \", best_features)\n"
      ],
      "metadata": {
        "id": "f6MHjq9LsT3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
        "# This assumes merged_df is your PySpark DataFrame\n",
        "merged_df_pd = merged_df.toPandas()  # Convert to Pandas DataFrame\n",
        "\n",
        "# Prepare the data\n",
        "features = ['artist_count', 'gender', 'length', 'level', 'thumb_up', 'thumb_down', 'song_count']\n",
        "target = 'target'  # Replace with your actual target column name\n",
        "\n",
        "X = merged_df_pd[features].values\n",
        "y = merged_df_pd[target].values\n",
        "\n",
        "# Split the data into training and test sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape the data for LSTM [samples, time steps, features]\n",
        "# Here, we treat each feature set as a time step of 1\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(X_train_reshaped.shape[1], 1)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "RZxRRxCrDIFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU, Dense, Dropout, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic data for demonstration\n",
        "def generate_data(num_samples, timesteps, features):\n",
        "    # Random data for example purposes\n",
        "    X = np.random.rand(num_samples, timesteps, features)\n",
        "    y = np.random.randint(0, 2, num_samples)  # Binary labels\n",
        "    return X, y\n",
        "\n",
        "# Parameters\n",
        "num_samples = 1000  # Number of samples\n",
        "timesteps = 10      # Number of time steps in each sample\n",
        "features = 5        # Number of features at each time step\n",
        "\n",
        "# Generate data\n",
        "X, y = generate_data(num_samples, timesteps, features)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data (optional, depending on your dataset)\n",
        "scaler = StandardScaler()\n",
        "X_train_reshaped = X_train.reshape(-1, timesteps * features)\n",
        "X_test_reshaped = X_test.reshape(-1, timesteps * features)\n",
        "X_train_scaled = scaler.fit_transform(X_train_reshaped).reshape(X_train.shape)\n",
        "X_test_scaled = scaler.transform(X_test_reshaped).reshape(X_test.shape)\n",
        "\n",
        "# Create the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(Input(shape=(timesteps, features)))\n",
        "\n",
        "# Add an LSTM layer\n",
        "model.add(LSTM(units=64, return_sequences=True))\n",
        "\n",
        "# Add a GRU layer\n",
        "model.add(GRU(units=32, return_sequences=False))\n",
        "\n",
        "# Optional: Add a Dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # For binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Plotting training history (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SGlblBWAMPC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU, Dense, Dropout, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Generate synthetic data for demonstration\n",
        "def generate_data(num_samples, timesteps, features):\n",
        "    # Random data for example purposes\n",
        "    X = np.random.rand(num_samples, timesteps, features)\n",
        "    y = np.random.randint(0, 2, num_samples)  # Binary labels\n",
        "    return X, y\n",
        "\n",
        "# Parameters\n",
        "num_samples = 1000  # Number of samples\n",
        "timesteps = 10      # Number of time steps in each sample\n",
        "features = 5        # Number of features at each time step\n",
        "\n",
        "# Generate data\n",
        "X, y = generate_data(num_samples, timesteps, features)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data (optional, depending on your dataset)\n",
        "scaler = StandardScaler()\n",
        "X_train_reshaped = X_train.reshape(-1, timesteps * features)\n",
        "X_test_reshaped = X_test.reshape(-1, timesteps * features)\n",
        "X_train_scaled = scaler.fit_transform(X_train_reshaped).reshape(X_train.shape)\n",
        "X_test_scaled = scaler.transform(X_test_reshaped).reshape(X_test.shape)\n",
        "\n",
        "# Create the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(Input(shape=(timesteps, features)))\n",
        "\n",
        "# Add an LSTM layer\n",
        "model.add(LSTM(units=64, return_sequences=True))\n",
        "model.add(Dropout(0.2))  # Dropout after LSTM\n",
        "\n",
        "# Add a GRU layer\n",
        "model.add(GRU(units=32, return_sequences=False))\n",
        "model.add(Dropout(0.2))  # Dropout after GRU\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # For binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Plotting training history (optional)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TAOSIsVdOBp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU, Dense, Dropout, Input, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Generate synthetic data for demonstration\n",
        "def generate_data(num_samples, timesteps, features):\n",
        "    X = np.random.rand(num_samples, timesteps, features)\n",
        "    y = np.random.randint(0, 2, num_samples)  # Binary labels\n",
        "    return X, y\n",
        "\n",
        "# Parameters\n",
        "num_samples = 1000\n",
        "timesteps = 10\n",
        "features = 5\n",
        "\n",
        "# Generate data\n",
        "X, y = generate_data(num_samples, timesteps, features)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_reshaped = X_train.reshape(-1, timesteps * features)\n",
        "X_test_reshaped = X_test.reshape(-1, timesteps * features)\n",
        "X_train_scaled = scaler.fit_transform(X_train_reshaped).reshape(X_train.shape)\n",
        "X_test_scaled = scaler.transform(X_test_reshaped).reshape(X_test.shape)\n",
        "\n",
        "# Create the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(Input(shape=(timesteps, features)))\n",
        "\n",
        "# Bidirectional LSTM Layer\n",
        "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Bidirectional GRU Layer\n",
        "model.add(Bidirectional(GRU(units=32, return_sequences=False)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Plotting training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L9S441EVOcA2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
